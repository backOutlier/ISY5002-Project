{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1276317,"sourceType":"datasetVersion","datasetId":735911},{"sourceId":9637535,"sourceType":"datasetVersion","datasetId":5884518},{"sourceId":96990114,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchvision\nfrom torch import nn","metadata":{"id":"jdvoPWntkj2x","execution":{"iopub.status.busy":"2024-10-17T06:19:47.561372Z","iopub.execute_input":"2024-10-17T06:19:47.561812Z","iopub.status.idle":"2024-10-17T06:19:47.566437Z","shell.execute_reply.started":"2024-10-17T06:19:47.561770Z","shell.execute_reply":"2024-10-17T06:19:47.565325Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n# !mkdir -p ~/.kaggle\n\n# from google.colab import files\n# uploaded = files.upload()\n\n# !cp kaggle.json ~/.kaggle/\n\n# !chmod 600 ~/.kaggle/kaggle.json","metadata":{"id":"SxPDbb64qoqc","outputId":"b22b3129-a082-4f05-fbd1-ee45657348de","execution":{"iopub.status.busy":"2024-10-17T06:19:47.567899Z","iopub.execute_input":"2024-10-17T06:19:47.568202Z","iopub.status.idle":"2024-10-17T06:19:47.579188Z","shell.execute_reply.started":"2024-10-17T06:19:47.568171Z","shell.execute_reply":"2024-10-17T06:19:47.578438Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# !kaggle datasets download -d shubhamgoel27/dermnet\n\n# !unzip dermnet.zip -d dermnet","metadata":{"id":"1LwBaTbxqqMf","outputId":"5cab55d8-12a5-44cc-e90b-2bdc9df4a161","execution":{"iopub.status.busy":"2024-10-17T06:19:47.580926Z","iopub.execute_input":"2024-10-17T06:19:47.581352Z","iopub.status.idle":"2024-10-17T06:19:47.589626Z","shell.execute_reply.started":"2024-10-17T06:19:47.581294Z","shell.execute_reply":"2024-10-17T06:19:47.588905Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# !ls dermnet","metadata":{"id":"Mu9mYW17qr-W","outputId":"05ce5bc0-666f-44a5-af8a-3b5d99612a02","execution":{"iopub.status.busy":"2024-10-17T06:19:47.590615Z","iopub.execute_input":"2024-10-17T06:19:47.590892Z","iopub.status.idle":"2024-10-17T06:19:47.602246Z","shell.execute_reply.started":"2024-10-17T06:19:47.590860Z","shell.execute_reply":"2024-10-17T06:19:47.601219Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor","metadata":{"id":"T_TrOwmhqy-v","execution":{"iopub.status.busy":"2024-10-17T06:19:47.604582Z","iopub.execute_input":"2024-10-17T06:19:47.604993Z","iopub.status.idle":"2024-10-17T06:19:47.614460Z","shell.execute_reply.started":"2024-10-17T06:19:47.604947Z","shell.execute_reply":"2024-10-17T06:19:47.613697Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# @title Default title text\nimport os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# 定义预处理步骤\ntransform_train = transforms.Compose([\n    transforms.Resize((180, 180)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((180, 180)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# 定义你想要加载的类别\nselected_folders = ['Acne and Rosacea Photos',\n 'Actinic Keratosis Basal Cell Carcinoma and other Malignant Lesions',\n 'Atopic Dermatitis Photos',\n 'Bullous Disease Photos',\n 'Cellulitis Impetigo and other Bacterial Infections',\n 'Tinea Ringworm Candidiasis and other Fungal Infections',\n  'Melanoma Skin Cancer Nevi and Moles',\n 'Eczema Photos']  # 替换为你想加载的类别\n\n# train_data_dir = '/content/dermnet/train'\n# test_data_dir = '/content/dermnet/test'\ntrain_data_dir = '/kaggle/input/dermnet/train'\ntest_data_dir = '/kaggle/input/dermnet/test'\n\n# 加载完整的训练数据和测试数据\ndf_train_full = datasets.ImageFolder(root=train_data_dir, transform=transform_train)\ndf_test_full = datasets.ImageFolder(root=test_data_dir, transform=transform_test)\n\n# 获取类别名与索引的映射\n# Original: class_to_idx = {v: k for k, v in df_train_full.class_to_idx.items()}  # 反转字典\n# Changed: Using the original class_to_idx where keys are class names and values are indices\nclass_to_idx = df_train_full.class_to_idx\n\n# 找出选定类别对应的索引\n# Original: selected_idx = [class_to_idx[folder] for folder in selected_folders]\n# Changed: Accessing the index using the class name as the key\nselected_idx = [class_to_idx[folder] for folder in selected_folders if folder in class_to_idx]\n\n# 过滤出选定类别的数据索引\ntrain_indices = [i for i, (_, label) in enumerate(df_train_full) if label in selected_idx]\ntest_indices = [i for i, (_, label) in enumerate(df_test_full) if label in selected_idx]\n\n# 使用 Subset 只加载指定类别的数据\ndf_train_subset = Subset(df_train_full, train_indices)\ndf_test_subset = Subset(df_test_full, test_indices)\n\n# DataLoader\nbatch_size = 64\ntrain_data_loader = DataLoader(df_train_subset, batch_size=batch_size, shuffle=True)\ntest_data_loader = DataLoader(df_test_subset, batch_size=batch_size, shuffle=False)\n\n# 查看加载数据的类别\nclass_names = [folder for folder in df_train_full.classes if folder in selected_folders]\nprint(class_names)","metadata":{"cellView":"form","id":"PQhlIYt7rVSt","execution":{"iopub.status.busy":"2024-10-17T06:19:47.615439Z","iopub.execute_input":"2024-10-17T06:19:47.615727Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Default title text\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\ntransform_train = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n\n# train_data_dir = '/content/dermnet/train'\n# test_data_dir = '/content/dermnet/test'\ntrain_data_dir = '/kaggle/input/dermnet/train'\ntest_data_dir = '/kaggle/input/dermnet/test'\ndf_train = datasets.ImageFolder(root=train_data_dir, transform=transform_train)\ndf_test = datasets.ImageFolder(root=test_data_dir,transform=transform_test)\n\n# DataLoader\nbatch_size = 64\ntrain_data_loader = DataLoader(df_train, batch_size=batch_size, shuffle=True)\ntest_data_loader = DataLoader(df_test, batch_size=batch_size, shuffle=False)\n\n\n\n# show a batch data\nimages, labels = next(iter(train_data_loader))\nprint(f'Image batch shape: {images.shape}')\nprint(f'Label batch shape: {labels.shape}')\n\nclass_names = df_train.classes\nprint(class_names)","metadata":{"id":"j_CMGGUPyXFA","outputId":"348509ad-a555-408d-92b4-14263956e01e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data_loader\ntrain_data_loader","metadata":{"id":"QT-4pA_WsVFA","outputId":"c06b0ea2-a4d1-4ddb-9c93-0862c9b5fd40","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Default title text\n!pip install torch torchvision\n!pip install efficientnet-pytorch","metadata":{"id":"CcMzTj6pzYn1","outputId":"83bc799d-4033-4eaf-a5c4-adf8276ce2ca","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import ImageFolder","metadata":{"id":"C8OKzZJ0zySP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"5i-j_-cwv7mz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#需要改动换掉这个类，其余的正常运行就好\nclass CustomVGG(nn.Module): #VGG16\n    def __init__(self, num_classes):  \n        super(CustomVGG, self).__init__()  \n        # load VGG16  \n        self.base_model = models.vgg16(pretrained=True)  \n          \n        # Remove the last fully connected layer  \n        self.base_model.classifier[6] = nn.Identity()  # The last layer in VGG's classifier is a fully connected layer  \n          \n        # Adapt the input size of the new fully connected layers  \n        # VGG16 outputs 4096 features (after the adaptive average pooling and flattening) from its last convolutional + pooling layers  \n        self.fc1 = nn.Linear(4096, 512)  \n        self.fc2 = nn.Linear(512, num_classes)  \n          \n        self.relu = nn.ReLU()  \n  \n    def forward(self, x):  \n        x = self.base_model(x)  \n        x = x.view(x.size(0), -1)  # Flatten the tensor. This is usually done automatically by the fully connected layer, but we do it explicitly here for clarity.  \n        x = self.relu(self.fc1(x))  \n        x = self.fc2(x)  \n        return x\n\nnum_classes = 23\nmodel = CustomVGG(num_classes=num_classes).to(device)","metadata":{"id":"GdsZpy_KyhfX","outputId":"afe5b49d-fc36-41f4-e02f-c3bae5f62fcf","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"1EVyBG0kzpCE"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)","metadata":{"id":"JITg75juy78k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"sWIy10bdK2hF"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch.optim as optim\n\n# 定义早停法参数  \nearly_stopping_patience = 10  \nearly_stopping_counter = 0  \nbest_val_loss = float('inf')  \nstop_training = False  # 手动停止训练的标志  \n  \n# 定义ReduceLROnPlateau调度器  \nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)  \n  \nnum_epochs = 70  \ntrain_losses = []  \ntrain_accuracies = []  \nval_losses = []  \nval_accuracies = []  \n  \nfig, axs = plt.subplots(1, 2, figsize=(12, 5))  \nloss_line1, = axs[0].plot([], [], label='Train Loss')  \nloss_line2, = axs[0].plot([], [], label='Validation Loss')  \nacc_line1, = axs[1].plot([], [], label='Train Accuracy')  \nacc_line2, = axs[1].plot([], [], label='Validation Accuracy')  \n  \ndef init():  \n    for line in [loss_line1, loss_line2, acc_line1, acc_line2]:  \n        line.set_data([], [])  \n    return loss_line1, loss_line2, acc_line1, acc_line2  \n  \ndef update(epoch):  \n    axs[0].set_xlim(0, num_epochs)  \n    axs[0].set_ylim(0, max(max(train_losses), max(val_losses)) * 1.1 if train_losses and val_losses else 1)  \n    axs[1].set_ylim(0, 100)  \n      \n    loss_line1.set_data(range(1, epoch + 1), train_losses)  \n    loss_line2.set_data(range(1, epoch + 1), val_losses)  \n    acc_line1.set_data(range(1, epoch + 1), train_accuracies)  \n    acc_line2.set_data(range(1, epoch + 1), val_accuracies)  \n      \n    return loss_line1, loss_line2, acc_line1, acc_line2  \n  \nani = animation.FuncAnimation(fig, update, frames=num_epochs, init_func=init, blit=True, interval=1000)  # interval以毫秒为单位  \nplt.show(block=False)  # 非阻塞模式显示动画  \n  \nfor epoch in range(num_epochs): \n    # 训练代码（与原始代码相同，略去） \n    model.train()    \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for i, (images, labels) in enumerate(train_data_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        if (i + 1) % 5 == 0:\n            batch_loss = running_loss / (i + 1)\n            batch_accuracy = 100 * correct / total\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], '\n                  f'Train Loss: {batch_loss:.4f}, Train Accuracy: {batch_accuracy:.2f}%')\n        \n    avg_loss = running_loss / len(train_data_loader)\n    avg_accuracy = 100 * correct / total\n    train_losses.append(avg_loss)\n    train_accuracies.append(avg_accuracy)\n    print(f'Epoch [{epoch + 1}/{num_epochs}] completed. Average Train Loss: {avg_loss:.4f}, '\n          f'Average Train Accuracy: {avg_accuracy:.2f}%')\n    # 验证代码（与原始代码相同，略去）  \n    model.eval()\n    val_running_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for images, labels in test_data_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            val_running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_running_loss / len(test_data_loader)\n    avg_val_accuracy = 100 * val_correct / val_total\n    val_losses.append(avg_val_loss)\n    val_accuracies.append(avg_val_accuracy)\n\n    print(f'Epoch [{epoch + 1}/{num_epochs}] completed. Validation Loss: {avg_val_loss:.4f}, '\n          f'Validation Accuracy: {avg_val_accuracy:.2f}%')  \n    # 更新学习率调度器  \n    scheduler.step(avg_val_loss)  \n      \n    # 早停法逻辑  \n    if avg_val_loss < best_val_loss:  \n        best_val_loss = avg_val_loss  \n        early_stopping_counter = 0  \n    else:  \n        early_stopping_counter += 1  \n      \n    # 检查是否应该停止训练  \n    if early_stopping_counter >= early_stopping_patience or stop_training:  \n        print(f'Early stopping at epoch {epoch + 1} due to no improvement in validation loss.')  \n        break  \n      \n    # 更新动画（仅在本地运行时有效，Kaggle等在线环境不支持）  \n    try:  \n        ani.event_source.stop()  # 停止动画更新（如果之前已经开始的话）  \n        ani.event_source = ani.new_timer(interval=1000)  # 重新设置定时器以继续更新（如果需要的话，但在这里我们其实不需要，因为下面会重新显示）  \n        plt.close(fig)  # 关闭当前图形窗口（为了避免多个窗口同时打开）  \n        fig, axs = plt.subplots(1, 2, figsize=(12, 5))  # 重新创建图形和轴  \n        # 重新初始化动画（略去，因为下面我们会保存并重新显示整个动画）  \n        # ...  \n        # 但实际上，在这里我们不需要重新初始化动画，因为我们会直接保存图像并退出循环  \n    except:  \n        pass  # 忽略在线环境不支持动画更新时的错误  \n      \n# 训练结束后保存曲线图（无论是因为完成所有epochs还是因为早停）  \nplt.figure(figsize=(12, 5))  \nplt.subplot(1, 2, 1)  \nplt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')  \nplt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')  \nplt.xlabel('Epoch')  \nplt.ylabel('Loss')  \nplt.title('Training and Validation Loss')  \nplt.legend()  \n  \nplt.subplot(1, 2, 2)  \nplt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')  \nplt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')  \nplt.xlabel('Epoch')  \nplt.ylabel('Accuracy (%)')  \nplt.title('Training and Validation Accuracy')  \nplt.legend()  \n  \nplt.tight_layout()  \nplt.savefig('training_curves.png')  # 保存曲线图  \nplt.show()  # 在本地显示曲线图（Kaggle等在线环境可能不支持显示）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import torch.optim as optim\n\n# # define ReduceLROnPlateau schedule\n# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n\n# num_epochs = 70\n# train_losses = []\n# train_accuracies = []\n# val_losses = []\n# val_accuracies = []\n\n# for epoch in range(num_epochs):\n#     model.train()\n#     running_loss = 0.0\n#     correct = 0\n#     total = 0\n#     for i, (images, labels) in enumerate(train_data_loader):\n#         images, labels = images.to(device), labels.to(device)\n\n#         optimizer.zero_grad()\n#         outputs = model(images)\n#         loss = criterion(outputs, labels)\n\n#         loss.backward()\n#         optimizer.step()\n\n#         running_loss += loss.item()\n#         _, predicted = torch.max(outputs.data, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n\n#         if (i + 1) % 5 == 0:\n#             batch_loss = running_loss / (i + 1)\n#             batch_accuracy = 100 * correct / total\n#             print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_data_loader)}], '\n#                   f'Train Loss: {batch_loss:.4f}, Train Accuracy: {batch_accuracy:.2f}%')\n\n#     avg_loss = running_loss / len(train_data_loader)\n#     avg_accuracy = 100 * correct / total\n#     train_losses.append(avg_loss)\n#     train_accuracies.append(avg_accuracy)\n#     print(f'Epoch [{epoch + 1}/{num_epochs}] completed. Average Train Loss: {avg_loss:.4f}, '\n#           f'Average Train Accuracy: {avg_accuracy:.2f}%')\n\n#     # validation\n#     model.eval()\n#     val_running_loss = 0.0\n#     val_correct = 0\n#     val_total = 0\n#     with torch.no_grad():\n#         for images, labels in test_data_loader:\n#             images, labels = images.to(device), labels.to(device)\n#             outputs = model(images)\n#             loss = criterion(outputs, labels)\n\n#             val_running_loss += loss.item()\n#             _, predicted = torch.max(outputs.data, 1)\n#             val_total += labels.size(0)\n#             val_correct += (predicted == labels).sum().item()\n\n#     avg_val_loss = val_running_loss / len(test_data_loader)\n#     avg_val_accuracy = 100 * val_correct / val_total\n#     val_losses.append(avg_val_loss)\n#     val_accuracies.append(avg_val_accuracy)\n\n#     print(f'Epoch [{epoch + 1}/{num_epochs}] completed. Validation Loss: {avg_val_loss:.4f}, '\n#           f'Validation Accuracy: {avg_val_accuracy:.2f}%')\n\n#     # ReduceLROnPlateau learning rate\n#     scheduler.step(avg_val_loss)\n\n# # loss diagram\n# plt.figure(figsize=(12, 5))\n# plt.subplot(1, 2, 1)\n# plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n# plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.title('Training and Validation Loss')\n# plt.legend()\n\n# # accuracy diagram\n# plt.subplot(1, 2, 2)\n# plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n# plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n# plt.xlabel('Epoch')\n# plt.ylabel('Accuracy (%)')\n# plt.title('Training and Validation Accuracy')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"id":"NF-6zjjzzSFG","outputId":"02b68082-3048-478f-a85b-a9afbfc51eab","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save\n# torch.save(model.state_dict(), 'resnet50_skin_disease_model.pth')\ntorch.save(model, 'vgg16_skin_disease_model.pth')","metadata":{"id":"pqBMWWT8zBWw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"dDwVxgppQl7y"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# from google.colab import files\n# files.download('vgg16_skin_disease_model.pth')\n","metadata":{"id":"7c5W7dawL8-B","outputId":"05fe7703-f066-4974-8587-eff21d514c1b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for images, labels in test_data_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f'Test Accuracy: {accuracy:.2f}%')\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_true = []\ny_pred = []\n\nwith torch.no_grad():\n    for images, labels in test_data_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        y_true.extend(labels.cpu().numpy())\n        y_pred.extend(predicted.cpu().numpy())\n\n# classifier report\nprint(classification_report(y_true, y_pred, target_names=class_names))\n\n# confusion matrix\ncm = confusion_matrix(y_true, y_pred)\n\n# The confusion matrix heat map\nplt.figure(figsize=(10, 8))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"id":"LHnfUA51Lb2L","outputId":"5f32a40f-5746-48bb-e6b5-38c930db1d0e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{"id":"RZjfBwx38P09"}}]}